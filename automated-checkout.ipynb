{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2e7d62b-5779-4211-822c-457c77321f8b",
   "metadata": {},
   "source": [
    "# Automated checkout using yolov8, text detection and recognition models with OpenVINO™\n",
    "\n",
    "This Jupyter notebook demonstrates an automated checkout system that integrates object detection and text detection/recognition AI models. Utilizing YOLOv8 for object detection, the notebook efficiently identifies various items within a checkout frame, leveraging its robust and precise detection capabilities. To complement this, the notebook incorporates text detection and recognition within designated regions of interest. This combination allows for the extraction and interpretation of textual information—such as product labels and prices—from the detected items, facilitating a seamless and automated checkout process. The notebook is designed with detailed code explanations and visual outputs, making it accessible for users to understand and adapt the technology for various retail environments.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Download the necessary models\n",
    "- Convert the models to OpenVINO IR.\n",
    "- Load models to OpenVINO engine \n",
    "- Live demo\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Get PyTorch model](#Get-PyTorch-model)\n",
    "    - [Prerequisites](#Prerequisites)\n",
    "- [Instantiate model](#Instantiate-model)\n",
    "    - [Convert model to OpenVINO IR](#Convert-model-to-OpenVINO-IR)\n",
    "    - [Verify model inference](#Verify-model-inference)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Test on single image](#Test-on-single-image)\n",
    "- [Live demo](#Live-demo)\n",
    "    - [Run Live Object Detection](#Run-Live-Object-Detection)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a12678-b12f-48d1-9735-398855733e46",
   "metadata": {},
   "source": [
    "## Get PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "We will use the YOLOv8 nano model (also known as `yolov8n`) pre-trained on a COCO dataset, which is available in this [repo](https://github.com/ultralytics/ultralytics). Similar steps are also applicable to other YOLOv8 models.\n",
    "Typical steps to obtain a pre-trained model:\n",
    "1. Create an instance of a model class.\n",
    "2. Load a checkpoint state dict, which contains the pre-trained model weights.\n",
    "3. Turn the model to evaluation for switching some operations to inference mode.\n",
    "\n",
    "In this case, the creators of the model provide an API that enables converting the YOLOv8 model to ONNX and then to OpenVINO IR. Therefore, we do not need to do these steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e654c7f-3b18-4cbe-9c54-c1fc490be604",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d04872-6916-454c-9211-6c644b50dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openvino-dev>=2024.0.0\" \"nncf>=2.9.0\" paho-mqtt ipywidgets Pillow numpy\n",
    "%pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"ultralytics==8.1.42\" onnx \"opencv-python==4.9.0.80\" tqdm --extra-index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bbe319c",
   "metadata": {},
   "source": [
    "Import required utility functions.\n",
    "The lower cell will download the `notebook_utils` Python module from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f6cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import download_file, VideoPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009cd9a-0670-44d8-8c7b-7b9faff16f11",
   "metadata": {},
   "source": [
    "## Download Horizontal Text Detection and Text Recognition resnet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa2c5ef-c99c-485d-b6b5-0b39391c5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "model_dir = Path(\"model\")\n",
    "precision = \"FP16\"\n",
    "detection_model = \"horizontal-text-detection-0001\"\n",
    "recognition_model = \"text-recognition-resnet-fc\"\n",
    "\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "download_command = (\n",
    "    f\"omz_downloader --name {detection_model},{recognition_model} --output_dir {model_dir} --cache_dir {model_dir} --precision {precision}  --num_attempts 5\"\n",
    ")\n",
    "display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "display(Markdown(f\"Downloading {detection_model}, {recognition_model}...\"))\n",
    "!$download_command\n",
    "display(Markdown(f\"Finished downloading {detection_model}, {recognition_model}.\"))\n",
    "\n",
    "detection_model_path = (model_dir / \"intel/horizontal-text-detection-0001\" / precision / detection_model).with_suffix(\".xml\")\n",
    "recognition_model_path = (model_dir / \"public/text-recognition-resnet-fc\" / precision / recognition_model).with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea08b1-5864-4721-979d-822ecbc5e3c9",
   "metadata": {},
   "source": [
    "## Convert Text Recognition Model to OpenVINO IR\n",
    "\n",
    "The downloaded detection model is an Intel model, which is already in OpenVINO Intermediate Representation (OpenVINO IR) format. The text recognition model is a public model which needs to be converted to OpenVINO IR. Since this model was downloaded from Open Model Zoo, use Model Converter to convert the model to OpenVINO IR format.\n",
    "\n",
    "The output of Model Converter will be displayed. When the conversion is successful, the last lines of output will include `[ SUCCESS ] Generated IR version 11 model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabf7a05-da9e-4b20-9af1-c815b5766bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_command = f\"omz_converter --name {recognition_model} --precisions {precision} --download_dir {model_dir} --output_dir {model_dir}\"\n",
    "display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "display(Markdown(f\"Converting {recognition_model}...\"))\n",
    "! $convert_command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee32fd08-650c-4751-bb41-d8afccb2495e",
   "metadata": {},
   "source": [
    "## Instantiate YoloV8 model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "There are [several models](https://docs.ultralytics.com/tasks/detect/) available in the original repository, targeted for different tasks. For loading the model, required to specify a path to the model checkpoint. It can be some local path or name available on models hub (in this case model checkpoint will be downloaded automatically). \n",
    "\n",
    "Making prediction, the model accepts a path to input image and returns list with Results class object. Results contains boxes for object detection model. Also it contains utilities for processing results, for example, `plot()` method for drawing.\n",
    "\n",
    "Let us consider the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae2543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8n\"\n",
    "\n",
    "det_model = YOLO(models_dir / f\"{DET_MODEL_NAME}.pt\")\n",
    "label_map = det_model.model.names\n",
    "\n",
    "res = det_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e345ffcc-c4b8-44ba-8b03-f37e63a060da",
   "metadata": {},
   "source": [
    "### Convert YoloV8 model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "YOLOv8 provides API for convenient model exporting to different formats including OpenVINO IR. `model.export` is responsible for model conversion. We need to specify the format, and additionally, we can preserve dynamic shapes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf2cc576-50c9-409f-be86-ad7122dce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection model\n",
    "det_model_path = models_dir / f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\"\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e135b-77a6-4b77-a27e-30b351f7d3d0",
   "metadata": {},
   "source": [
    "## Text detection functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980bd2f-140f-45ac-b97b-2d46090357a9",
   "metadata": {},
   "source": [
    "### Select inference device for text detection\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ae27e-473d-4fc6-9edc-cc43cbda3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "text_device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "text_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc6a0c5-1513-45ae-918f-5afe4557c208",
   "metadata": {},
   "source": [
    "### Load a Text Detection Model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7019f5-9de9-4d3b-b953-91423a03f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_detection_model = core.read_model(model=detection_model_path, weights=detection_model_path.with_suffix(\".bin\"))\n",
    "text_detection_compiled_model = core.compile_model(model=text_detection_model, device_name=text_device.value)\n",
    "text_detection_input_layer = text_detection_compiled_model.input(0)\n",
    "text_output_key = text_detection_compiled_model.output(\"boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae20258-5d89-4f72-922e-1a298b495cdc",
   "metadata": {},
   "source": [
    "### Load Pre and Post processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "404b923b-a17a-4305-bc99-6f87b5b84e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_ratio(ratio_x, ratio_y, box):\n",
    "    return [max(shape * ratio_y, 10) if idx % 2 else shape * ratio_x for idx, shape in enumerate(box[:-1])]\n",
    "\n",
    "\n",
    "def run_preprocesing_on_crop(crop, net_shape):\n",
    "    temp_img = cv2.resize(crop, net_shape)\n",
    "    temp_img = temp_img.reshape((1,) * 2 + temp_img.shape)\n",
    "    return temp_img\n",
    "\n",
    "\n",
    "# For each detection, the description is in the [x_min, y_min, x_max, y_max, conf] format:\n",
    "# The image passed here is in BGR format with changed width and height. To display it in colors expected by matplotlib, use cvtColor function\n",
    "def convert_result_to_image(input_img, resized_image, boxes, threshold=0.3, conf_labels=True):\n",
    "     # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0), \"white\": (255, 255, 255)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "            input_img.shape[:2],\n",
    "        resized_image.shape[:2],\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    rgb_image = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "    # Iterate through non-zero boxes.\n",
    "    for box, annotation in boxes:\n",
    "        \n",
    "        # Pick a confidence factor from the last place in an array.                \n",
    "        conf = box[-1]                \n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply position of each box by x and y ratio.\n",
    "            (x_min, y_min, x_max, y_max) = map(int, multiply_by_ratio(ratio_x, ratio_y, box))\n",
    "\n",
    "            # Draw a box based on the position. Parameters in the `rectangle` function are: image, start_point, end_point, color, thickness.\n",
    "            cv2.rectangle(input_img, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add a text to an image based on the position and confidence. Parameters in the `putText` function are: image, text, bottomleft_corner_textfield, font, font_scale, color, thickness, line_type\n",
    "            if conf_labels:\n",
    "                # Create a background box based on annotation length.\n",
    "                (text_w, text_h), _ = cv2.getTextSize(f\"{annotation}\", cv2.FONT_HERSHEY_TRIPLEX, 0.8, 1)\n",
    "                image_copy = input_img.copy()\n",
    "                cv2.rectangle(\n",
    "                    image_copy,\n",
    "                    (x_min, y_min - text_h - 10),\n",
    "                    (x_min + text_w, y_min - 10),\n",
    "                    colors[\"white\"],\n",
    "                    -1,\n",
    "                )\n",
    "                # Add weighted image copy with white boxes under a text.\n",
    "                cv2.addWeighted(image_copy, 0.4, rgb_image, 0.6, 0, rgb_image)\n",
    "                cv2.putText(\n",
    "                    input_img,\n",
    "                    f\"{annotation}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f21d3-8547-46c9-ad18-74c61b82ac35",
   "metadata": {},
   "source": [
    "## Text Recognition\n",
    "\n",
    "Load the text recognition model and do inference on the detected boxes from the detection model.\n",
    "\n",
    "### Load Text Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33ce651c-6571-474f-9f7f-7fec57b010ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_model = core.read_model(model=recognition_model_path, weights=recognition_model_path.with_suffix(\".bin\"))\n",
    "\n",
    "recognition_compiled_model = core.compile_model(model=recognition_model, device_name=text_device.value)\n",
    "\n",
    "recognition_output_layer = recognition_compiled_model.output(0)\n",
    "recognition_input_layer = recognition_compiled_model.input(0)\n",
    "\n",
    "# Get the height and width of the input layer.\n",
    "_, _, H, W = recognition_input_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e6d6a-54a4-4a08-ba32-ebf60bd1925b",
   "metadata": {},
   "source": [
    "### Load inference text recognition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad0de5f-c7ee-4de3-bf82-d5068ceb29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_recognition_inferece(image,resized_image,boxes):\n",
    "    if boxes is None or len(boxes) == 0:        \n",
    "        return []\n",
    "    # Calculate scale for image resizing.\n",
    "    (real_y, real_x), (resized_y, resized_x) = image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "    \n",
    "    # Convert the image to grayscale for the text recognition model.\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get a dictionary to encode output, based on the model documentation.\n",
    "    letters = \"~0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "    \n",
    "    # Prepare an empty list for annotations.\n",
    "    annotations = list()\n",
    "    cropped_images = list()\n",
    "    # fig, ax = plt.subplots(len(boxes), 1, figsize=(5,15), sharex=True, sharey=True)\n",
    "    # Get annotations for each crop, based on boxes given by the detection model.\n",
    "    for i, crop in enumerate(boxes):\n",
    "        # Get coordinates on corners of a crop.\n",
    "        (x_min, y_min, x_max, y_max) = map(int, multiply_by_ratio(ratio_x, ratio_y, crop))\n",
    "        image_crop = run_preprocesing_on_crop(grayscale_image[y_min:y_max, x_min:x_max], (W, H))\n",
    "    \n",
    "        # Run inference with the recognition model.\n",
    "        result = recognition_compiled_model([image_crop])[recognition_output_layer]\n",
    "        \n",
    "        # Squeeze the output to remove unnecessary dimension.\n",
    "        recognition_results_test = np.squeeze(result)\n",
    "    \n",
    "        # Read an annotation based on probabilities from the output layer.\n",
    "        annotation = list()\n",
    "        for letter in recognition_results_test:\n",
    "            parsed_letter = letters[letter.argmax()]\n",
    "    \n",
    "            # Returning 0 index from `argmax` signalizes an end of a string.\n",
    "            if parsed_letter == letters[0]:\n",
    "                break\n",
    "            annotation.append(parsed_letter)\n",
    "        annotations.append(\"\".join(annotation))\n",
    "        cropped_image = Image.fromarray(image[y_min:y_max, x_min:x_max])\n",
    "        cropped_images.append(cropped_image)\n",
    "    \n",
    "    boxes_with_annotations = list(zip(boxes, annotations))\n",
    "    return boxes_with_annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e3b4862-c182-4ce4-a473-9f38d98deab8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Live demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The following code runs model inference on a video:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759bb6bc-1ff2-4d42-a493-1af38ec0e5f5",
   "metadata": {},
   "source": [
    "### Load function to draw a region of interest (ROI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8583f56e-0566-4acf-aa36-0c10907d76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_points = []\n",
    "drawing = False  # True if the mouse is pressed\n",
    "ix, iy = -1, -1\n",
    "\n",
    "global frame,original_frame\n",
    "\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global ix, iy, drawing, frame, original_frame\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix, iy = x, y  # Save the start point\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            temp_frame = original_frame.copy()\n",
    "            cv2.rectangle(temp_frame, (ix, iy), (x, y), (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Video\", temp_frame)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        rect_points[:] = [(ix, iy), (x, y)]  # Save the rectangle coordinates\n",
    "        cv2.rectangle(frame, (ix, iy), (x, y), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56ea00-c00c-4a4b-9854-14fa4d5c4099",
   "metadata": {},
   "source": [
    "### Load MQTT Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c18285-af6f-431d-8905-f2690c3e8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    if rc == 0:\n",
    "        print(\"Connected to MQTT Broker!\")\n",
    "    else:\n",
    "        print(\"Failed to connect, return code %d\\n\", rc)\n",
    "\n",
    "def on_disconnect(client, userdata, rc):\n",
    "    if rc != 0:\n",
    "        print(\"Unexpected disconnection.\")\n",
    "\n",
    "def connect_mqtt(broker_address, port, keepalive=60, client_id=\"\"):\n",
    "    # If client_id is an empty string or None, it should not be passed to Client()\n",
    "    if client_id:\n",
    "        client = mqtt.Client(client_id)\n",
    "    else:\n",
    "        client = mqtt.Client()  # Let paho generate a random client ID\n",
    "    \n",
    "    # Attach the connection and disconnection callbacks\n",
    "    client.on_connect = on_connect\n",
    "    client.on_disconnect = on_disconnect\n",
    "\n",
    "    # Connect to the MQTT broker\n",
    "    client.connect(\"127.0.0.1\", 1883, keepalive)\n",
    "\n",
    "    # Start the network loop in a separate thread\n",
    "    client.loop_start()\n",
    "    \n",
    "    return client\n",
    "\n",
    "def publish(client,data):\n",
    "    payload = json.dumps(data)\n",
    "    result = client.publish(\"ai/checkout\", payload)\n",
    "    # Check for successful publish\n",
    "    if result.rc != mqtt.MQTT_ERR_SUCCESS:\n",
    "        print(\"Failed to publish data.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7cf35-e944-4d65-b17a-1d0ba41b664d",
   "metadata": {},
   "source": [
    "### Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c7bb92e-e301-45a9-b5ff-f7953fad298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import openvino as ov\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def run_object_detection(\n",
    "    source=0,\n",
    "    flip=False,\n",
    "    use_popup=False,\n",
    "    skip_first_frames=0,\n",
    "    model=det_model,\n",
    "    device=device.value,\n",
    "    mqtt=None,\n",
    "):\n",
    "    global frame,original_frame\n",
    "    player = None\n",
    "    ov_config = {}\n",
    "    if device != \"CPU\":\n",
    "        model.reshape({0: [1, 3, 640, 640]})\n",
    "    if \"GPU\" in device or (\"AUTO\" in device and \"GPU\" in core.available_devices):\n",
    "        ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "    compiled_model = core.compile_model(model, device, ov_config)\n",
    "\n",
    "    def infer(*args):\n",
    "        result = compiled_model(args)\n",
    "        return torch.from_numpy(result[0])\n",
    "\n",
    "    det_model.predictor.inference = infer\n",
    "    det_model.predictor.model.pt = False\n",
    "\n",
    "    try:\n",
    "        # Connect to MQTT\n",
    "        mqtt_client = None\n",
    "        if mqtt is not None:           \n",
    "            mqttInfo = mqtt.split(\":\")\n",
    "            mqtt_client = connect_mqtt(mqttInfo[0], mqttInfo[1])\n",
    "        \n",
    "        # Create a video player to play with target fps.\n",
    "        player = VideoPlayer(source=source, flip=flip, fps=60, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        frame = player.next()\n",
    "            \n",
    "        if use_popup:\n",
    "            original_frame = frame.copy()\n",
    "            cv2.namedWindow(\"Video\", flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)            \n",
    "            cv2.setMouseCallback(\"Video\",draw_rectangle)\n",
    "            # Display the first frame and wait for the user to draw the rectangle\n",
    "            cv2.imshow(\"Video\", frame)\n",
    "            cv2.waitKey(0)  # Wait until any key is pressed\n",
    "\n",
    "        processing_times = collections.deque()  \n",
    "\n",
    "        # Blacklist object. Don't perform text detection and recognition on these objects. Update as needed.\n",
    "        blacklist = [\"person\", \"dining table\", \"sink\", \"cup\"]\n",
    "        \n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            \n",
    "            \n",
    "            if len(rect_points) == 2:\n",
    "                cv2.rectangle(frame, rect_points[0], rect_points[1], (0, 255, 0), 2)\n",
    "                # Crop the ROI\n",
    "                roi = frame[max(0, rect_points[0][1]):rect_points[1][1], max(0, rect_points[0][0]):rect_points[1][0]]\n",
    "            else:\n",
    "                roi = frame.copy()\n",
    "                        \n",
    "            input_image = np.array(roi)\n",
    "\n",
    "            # N,C,H,W = batch size, number of channels, height, width.\n",
    "            N, C, H, W = text_detection_input_layer.shape           \n",
    "\n",
    "            start_time = time.time()\n",
    "            # Object detection yolov8 inference\n",
    "            detections = det_model(input_image,verbose=False)\n",
    "           \n",
    "            # Initialize the list to hold JSON objects\n",
    "            data_list = []\n",
    "\n",
    "            # Perfom text detection and recognition on each detected object            \n",
    "            for det in detections[0].boxes:\n",
    "                xmin, ymin, xmax, ymax = map(int, det.xyxy[0].numpy())\n",
    "                conf = det.conf.item()  # Confidence\n",
    "                label = det_model.names[det.cls.item()]  # Class label from the detected object\n",
    "\n",
    "\n",
    "                # Check if bounding box is valid\n",
    "                if label not in blacklist:\n",
    "                    if xmin < xmax and ymin < ymax:\n",
    "                        object_roi = input_image[ymin:ymax, xmin:xmax]\n",
    "                            \n",
    "                        if object_roi.size != 0:  # Check if ROI is not empty\n",
    "                            # Perform text detection\n",
    "                            resized_image = cv2.resize(object_roi, (W, H))\n",
    "                            text_input_img = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "                            text_boxes = text_detection_compiled_model([text_input_img])[text_output_key]\n",
    "                            # Remove zero-only boxes.\n",
    "                            text_boxes = text_boxes[~np.all(text_boxes == 0, axis=1)]\n",
    "                                \n",
    "                            # Text recognition\n",
    "                            boxes_with_annotations = text_recognition_inferece(object_roi, resized_image, text_boxes)\n",
    "                            recognized_texts = [text for box, text in boxes_with_annotations]\n",
    "    \n",
    "                            # Append data for this detection to the list\n",
    "                            # Plotting Text results\n",
    "                            roi = convert_result_to_image(object_roi, resized_image, boxes_with_annotations, conf_labels=True)                                                                                                    \n",
    "                            json_object = {}\n",
    "                            json_object[\"confidence\"] = conf\n",
    "                            json_object[\"label\"] = label\n",
    "                            json_object[\"Text\"] = recognized_texts\n",
    "                            data_list.append(json_object)                            \n",
    "                                                                              \n",
    "                            print(f\"Confidence: {conf:.2f}, Label: {label}, Recognized Texts: {recognized_texts}\")\n",
    "                            \n",
    "                        else:\n",
    "                            print(\"Empty ROI detected, skipping this detection.\")\n",
    "                    else:\n",
    "                        print(\"Invalid bounding box coordinates detected, skipping this detection.\")\n",
    "\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            # Plotting object detection results\n",
    "            roi = detections[0].plot()            \n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            # Calculate mean processing time and FPS\n",
    "            f_height, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time \n",
    "\n",
    "            target_img = frame if use_popup else roi\n",
    "\n",
    "            \n",
    "            # initial_y_offset = 20\n",
    "            # line_spacing = 15\n",
    "\n",
    "            # for obj in data_list:\n",
    "            #     # Preparing text for display\n",
    "            #     info_text = f\"{obj['label']}, Texts: {', '.join(obj['Text'])}\"\n",
    "            #     text_size = cv2.getTextSize(info_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "            #     text_x = target_img.shape[1] - text_size[0] - 10  # 10 pixels from the right edge\n",
    "                            \n",
    "            #     # Place text in the top right corner\n",
    "            #     cv2.putText(target_img, info_text, (text_x, initial_y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            #     initial_y_offset += text_size[1] + line_spacing\n",
    "            \n",
    "            cv2.putText(\n",
    "                img=target_img,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )    \n",
    "            \n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                frame[max(0, rect_points[0][1]):rect_points[1][1], max(0, rect_points[0][0]):rect_points[1][0]] = roi\n",
    "                cv2.rectangle(target_img, rect_points[0], rect_points[1], (0, 255, 0), 2)\n",
    "                cv2.imshow(\"Video\",frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Convert to JPEG for display in Jupyter notebook.                \n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=roi, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                i = display.Image(data=encoded_img.tobytes())\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "\n",
    "            # Publish to MQTT\n",
    "            if mqtt is not None:\n",
    "                publish(mqtt_client,data_list)\n",
    "                data_list.clear()            \n",
    "    \n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()\n",
    "        if mqtt is not None:\n",
    "            mqtt_client.disconnect()\n",
    "            mqtt_client.loop_stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5b4ba6-f478-4417-b09d-93fee5adca41",
   "metadata": {},
   "source": [
    "### Run Live Object Detection\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Use a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`.\n",
    "\n",
    "Note: The region of interest feature only works when use_popup is set to True. This will open a window to draw the bounding box using your mouse and left click.\n",
    "\n",
    ">**NOTE**: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a remote server (for example, in Binder or Google Colab service), the webcam will not work. By default, the lower cell will run model inference on a video file. If you want to try live inference on your webcam set `WEBCAM_INFERENCE = True`\n",
    "\n",
    "Run the object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90708017",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM_INFERENCE = False\n",
    "\n",
    "if WEBCAM_INFERENCE:\n",
    "    VIDEO_SOURCE = 0  # Webcam\n",
    "else:\n",
    "    VIDEO_SOURCE = \"https://github.com/antoniomtz/sample-clips/raw/main/automated-small.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de91e4d-321f-46fe-a1ad-425e2a04b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_ov_model = core.read_model(det_model_path)\n",
    "\n",
    "run_object_detection(\n",
    "    source=VIDEO_SOURCE,\n",
    "    flip=False,\n",
    "    use_popup=False,\n",
    "    skip_first_frames=0,\n",
    "    model=det_ov_model,\n",
    "    device=device.value,\n",
    "    mqtt=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/212105105-f61c8aab-c1ff-40af-a33f-d0ed1fccc72e.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Object Detection"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
