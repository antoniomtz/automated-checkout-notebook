{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2e7d62b-5779-4211-822c-457c77321f8b",
   "metadata": {},
   "source": [
    "# Convert and Optimize YOLOv8 real-time object detection with OpenVINO™\n",
    "\n",
    "Real-time object detection is often used as a key component in computer vision systems.\n",
    "Applications that use real-time object detection models include video analytics, robotics, autonomous vehicles, multi-object tracking and object counting, medical image analysis, and many others.\n",
    "\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize PyTorch YOLOv8 with OpenVINO. We consider the steps required for object detection scenario.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Prepare the PyTorch model.\n",
    "- Download and prepare a dataset.\n",
    "- Validate the original model.\n",
    "- Convert the PyTorch model to OpenVINO IR.\n",
    "- Validate the converted model.\n",
    "- Prepare and run optimization pipeline.\n",
    "- Compare performance of the FP32 and quantized models.\n",
    "- Compare accuracy of the FP32 and quantized models.\n",
    "- Other optimization possibilities with OpenVINO api\n",
    "- Live demo\n",
    "\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Get PyTorch model](#Get-PyTorch-model)\n",
    "    - [Prerequisites](#Prerequisites)\n",
    "- [Instantiate model](#Instantiate-model)\n",
    "    - [Convert model to OpenVINO IR](#Convert-model-to-OpenVINO-IR)\n",
    "    - [Verify model inference](#Verify-model-inference)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Test on single image](#Test-on-single-image)\n",
    "- [Live demo](#Live-demo)\n",
    "    - [Run Live Object Detection](#Run-Live-Object-Detection)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7a12678-b12f-48d1-9735-398855733e46",
   "metadata": {},
   "source": [
    "## Get PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "We will use the YOLOv8 nano model (also known as `yolov8n`) pre-trained on a COCO dataset, which is available in this [repo](https://github.com/ultralytics/ultralytics). Similar steps are also applicable to other YOLOv8 models.\n",
    "Typical steps to obtain a pre-trained model:\n",
    "1. Create an instance of a model class.\n",
    "2. Load a checkpoint state dict, which contains the pre-trained model weights.\n",
    "3. Turn the model to evaluation for switching some operations to inference mode.\n",
    "\n",
    "In this case, the creators of the model provide an API that enables converting the YOLOv8 model to ONNX and then to OpenVINO IR. Therefore, we do not need to do these steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e654c7f-3b18-4cbe-9c54-c1fc490be604",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d04872-6916-454c-9211-6c644b50dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q \"openvino>=2024.0.0\" \"nncf>=2.9.0\"\n",
    "# %pip install -q \"torch>=2.1\" \"torchvision>=0.16\" \"ultralytics==8.1.42\" onnx tqdm opencv-python --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bbe319c",
   "metadata": {},
   "source": [
    "Import required utility functions.\n",
    "The lower cell will download the `notebook_utils` Python module from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f6cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import download_file, VideoPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373658bd-7e64-4479-914e-f2742d330afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/coco_bike.jpg' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/antonio/openvino_notebooks/notebooks/yolov8-optimization/data/coco_bike.jpg')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a test sample\n",
    "IMAGE_PATH = Path(\"./data/coco_bike.jpg\")\n",
    "download_file(\n",
    "    url=\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_bike.jpg\",\n",
    "    filename=IMAGE_PATH.name,\n",
    "    directory=IMAGE_PATH.parent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009cd9a-0670-44d8-8c7b-7b9faff16f11",
   "metadata": {},
   "source": [
    "## Dowload Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa2c5ef-c99c-485d-b6b5-0b39391c5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "model_dir = Path(\"model\")\n",
    "precision = \"FP16\"\n",
    "detection_model = \"horizontal-text-detection-0001\"\n",
    "recognition_model = \"text-recognition-resnet-fc\"\n",
    "\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# download_command = (\n",
    "#     f\"omz_downloader --name {detection_model},{recognition_model} --output_dir {model_dir} --cache_dir {model_dir} --precision {precision}  --num_attempts 5\"\n",
    "# )\n",
    "# display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "# display(Markdown(f\"Downloading {detection_model}, {recognition_model}...\"))\n",
    "# !$download_command\n",
    "# display(Markdown(f\"Finished downloading {detection_model}, {recognition_model}.\"))\n",
    "\n",
    "detection_model_path = (model_dir / \"intel/horizontal-text-detection-0001\" / precision / detection_model).with_suffix(\".xml\")\n",
    "recognition_model_path = (model_dir / \"public/text-recognition-resnet-fc\" / precision / recognition_model).with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57576720-3fb1-464c-9529-43217002ba3d",
   "metadata": {},
   "source": [
    "## Convert Models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The downloaded detection model is an Intel model, which is already in OpenVINO Intermediate Representation (OpenVINO IR) format. The text recognition model is a public model which needs to be converted to OpenVINO IR. Since this model was downloaded from Open Model Zoo, use Model Converter to convert the model to OpenVINO IR format.\n",
    "\n",
    "The output of Model Converter will be displayed. When the conversion is successful, the last lines of output will include `[ SUCCESS ] Generated IR version 11 model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd062b83-f506-46cc-a254-31ea7b2801e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_command = f\"omz_converter --name {recognition_model} --precisions {precision} --download_dir {model_dir} --output_dir {model_dir}\"\n",
    "# display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "# display(Markdown(f\"Converting {recognition_model}...\"))\n",
    "# ! $convert_command"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee32fd08-650c-4751-bb41-d8afccb2495e",
   "metadata": {},
   "source": [
    "## Instantiate YoloV8 model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "There are [several models](https://docs.ultralytics.com/tasks/detect/) available in the original repository, targeted for different tasks. For loading the model, required to specify a path to the model checkpoint. It can be some local path or name available on models hub (in this case model checkpoint will be downloaded automatically). \n",
    "\n",
    "Making prediction, the model accepts a path to input image and returns list with Results class object. Results contains boxes for object detection model. Also it contains utilities for processing results, for example, `plot()` method for drawing.\n",
    "\n",
    "Let us consider the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae2543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/antonio/openvino_notebooks/notebooks/yolov8-optimization/data/coco_bike.jpg: 480x640 2 bicycles, 2 cars, 1 dog, 37.6ms\n",
      "Speed: 1.6ms preprocess, 37.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8n\"\n",
    "\n",
    "det_model = YOLO(models_dir / f\"{DET_MODEL_NAME}.pt\")\n",
    "label_map = det_model.model.names\n",
    "\n",
    "res = det_model(IMAGE_PATH)\n",
    "# Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e345ffcc-c4b8-44ba-8b03-f37e63a060da",
   "metadata": {},
   "source": [
    "### Convert YoloV8 model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "YOLOv8 provides API for convenient model exporting to different formats including OpenVINO IR. `model.export` is responsible for model conversion. We need to specify the format, and additionally, we can preserve dynamic shapes in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2cc576-50c9-409f-be86-ad7122dce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection model\n",
    "det_model_path = models_dir / f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\"\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e135b-77a6-4b77-a27e-30b351f7d3d0",
   "metadata": {},
   "source": [
    "## Text detection functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980bd2f-140f-45ac-b97b-2d46090357a9",
   "metadata": {},
   "source": [
    "## Select inference device for text detection\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d2ae27e-473d-4fc6-9edc-cc43cbda3586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b2ad0200614bb18851091716d26df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "text_device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "text_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc6a0c5-1513-45ae-918f-5afe4557c208",
   "metadata": {},
   "source": [
    "### Load a Detection Model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7019f5-9de9-4d3b-b953-91423a03f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_detection_model = core.read_model(model=detection_model_path, weights=detection_model_path.with_suffix(\".bin\"))\n",
    "text_detection_compiled_model = core.compile_model(model=text_detection_model, device_name=text_device.value)\n",
    "text_detection_input_layer = text_detection_compiled_model.input(0)\n",
    "text_output_key = text_detection_compiled_model.output(\"boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366670e-91fc-4708-ab48-08f694bca6d8",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75e39c0-ec22-4a77-b33b-280b4ef42dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_ratio(ratio_x, ratio_y, box):\n",
    "    return [max(shape * ratio_y, 10) if idx % 2 else shape * ratio_x for idx, shape in enumerate(box[:-1])]\n",
    "\n",
    "\n",
    "def run_preprocesing_on_crop(crop, net_shape):\n",
    "    temp_img = cv2.resize(crop, net_shape)\n",
    "    temp_img = temp_img.reshape((1,) * 2 + temp_img.shape)\n",
    "    return temp_img\n",
    "\n",
    "\n",
    "# For each detection, the description is in the [x_min, y_min, x_max, y_max, conf] format:\n",
    "# The image passed here is in BGR format with changed width and height. To display it in colors expected by matplotlib, use cvtColor function\n",
    "def convert_result_to_image(input_img, resized_image, boxes, threshold=0.3, conf_labels=True):\n",
    "     # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0), \"white\": (255, 255, 255)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "            input_img.shape[:2],\n",
    "        resized_image.shape[:2],\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    rgb_image = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "    # Iterate through non-zero boxes.\n",
    "    for box, annotation in boxes:\n",
    "        \n",
    "        # Pick a confidence factor from the last place in an array.                \n",
    "        conf = box[-1]                \n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply position of each box by x and y ratio.\n",
    "            (x_min, y_min, x_max, y_max) = map(int, multiply_by_ratio(ratio_x, ratio_y, box))\n",
    "\n",
    "            # Draw a box based on the position. Parameters in the `rectangle` function are: image, start_point, end_point, color, thickness.\n",
    "            cv2.rectangle(input_img, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add a text to an image based on the position and confidence. Parameters in the `putText` function are: image, text, bottomleft_corner_textfield, font, font_scale, color, thickness, line_type\n",
    "            if conf_labels:\n",
    "                # Create a background box based on annotation length.\n",
    "                (text_w, text_h), _ = cv2.getTextSize(f\"{annotation}\", cv2.FONT_HERSHEY_TRIPLEX, 0.8, 1)\n",
    "                image_copy = input_img.copy()\n",
    "                cv2.rectangle(\n",
    "                    image_copy,\n",
    "                    (x_min, y_min - text_h - 10),\n",
    "                    (x_min + text_w, y_min - 10),\n",
    "                    colors[\"white\"],\n",
    "                    -1,\n",
    "                )\n",
    "                # Add weighted image copy with white boxes under a text.\n",
    "                cv2.addWeighted(image_copy, 0.4, rgb_image, 0.6, 0, rgb_image)\n",
    "                cv2.putText(\n",
    "                    input_img,\n",
    "                    f\"{annotation}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05554d0e-ea57-4708-a6f7-9a944fa758c4",
   "metadata": {},
   "source": [
    "## Text Recognition\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Load the text recognition model and do inference on the detected boxes from the detection model.\n",
    "\n",
    "### Load Text Recognition Model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4cee7a4-e05c-48ad-b554-d2f43c62accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_model = core.read_model(model=recognition_model_path, weights=recognition_model_path.with_suffix(\".bin\"))\n",
    "\n",
    "recognition_compiled_model = core.compile_model(model=recognition_model, device_name=text_device.value)\n",
    "\n",
    "recognition_output_layer = recognition_compiled_model.output(0)\n",
    "recognition_input_layer = recognition_compiled_model.input(0)\n",
    "\n",
    "# Get the height and width of the input layer.\n",
    "_, _, H, W = recognition_input_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e9187-1bee-4e35-bec6-a2a82fc8f638",
   "metadata": {},
   "source": [
    "### Load inference text recognition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf117ff7-f82c-49ec-a105-01ac7e433944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_recognition_inferece(image,resized_image,boxes):\n",
    "    # Calculate scale for image resizing.\n",
    "    (real_y, real_x), (resized_y, resized_x) = image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "    \n",
    "    # Convert the image to grayscale for the text recognition model.\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get a dictionary to encode output, based on the model documentation.\n",
    "    letters = \"~0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "    \n",
    "    # Prepare an empty list for annotations.\n",
    "    annotations = list()\n",
    "    cropped_images = list()\n",
    "    # fig, ax = plt.subplots(len(boxes), 1, figsize=(5,15), sharex=True, sharey=True)\n",
    "    # Get annotations for each crop, based on boxes given by the detection model.\n",
    "    for i, crop in enumerate(boxes):\n",
    "        # Get coordinates on corners of a crop.\n",
    "        (x_min, y_min, x_max, y_max) = map(int, multiply_by_ratio(ratio_x, ratio_y, crop))\n",
    "        image_crop = run_preprocesing_on_crop(grayscale_image[y_min:y_max, x_min:x_max], (W, H))\n",
    "    \n",
    "        # Run inference with the recognition model.\n",
    "        result = recognition_compiled_model([image_crop])[recognition_output_layer]\n",
    "        \n",
    "        # Squeeze the output to remove unnecessary dimension.\n",
    "        recognition_results_test = np.squeeze(result)\n",
    "    \n",
    "        # Read an annotation based on probabilities from the output layer.\n",
    "        annotation = list()\n",
    "        for letter in recognition_results_test:\n",
    "            parsed_letter = letters[letter.argmax()]\n",
    "    \n",
    "            # Returning 0 index from `argmax` signalizes an end of a string.\n",
    "            if parsed_letter == letters[0]:\n",
    "                break\n",
    "            annotation.append(parsed_letter)\n",
    "        annotations.append(\"\".join(annotation))\n",
    "        cropped_image = Image.fromarray(image[y_min:y_max, x_min:x_max])\n",
    "        cropped_images.append(cropped_image)\n",
    "    \n",
    "    boxes_with_annotations = list(zip(boxes, annotations))\n",
    "    return boxes_with_annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e3b4862-c182-4ce4-a473-9f38d98deab8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Live demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The following code runs model inference on a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b202ed8c-58c0-413f-845e-f4c8b9573019",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_points = []\n",
    "drawing = False  # True if the mouse is pressed\n",
    "ix, iy = -1, -1\n",
    "\n",
    "global frame,original_frame\n",
    "\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global ix, iy, drawing, frame, original_frame\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix, iy = x, y  # Save the start point\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            temp_frame = original_frame.copy()\n",
    "            cv2.rectangle(temp_frame, (ix, iy), (x, y), (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Video\", temp_frame)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        rect_points[:] = [(ix, iy), (x, y)]  # Save the rectangle coordinates\n",
    "        cv2.rectangle(frame, (ix, iy), (x, y), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c7bb92e-e301-45a9-b5ff-f7953fad298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import openvino as ov\n",
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def run_object_detection(\n",
    "    source=0,\n",
    "    flip=False,\n",
    "    use_popup=False,\n",
    "    skip_first_frames=0,\n",
    "    model=det_model,\n",
    "    device=device.value,\n",
    "):\n",
    "    global frame,original_frame\n",
    "    player = None\n",
    "    ov_config = {}\n",
    "    if device != \"CPU\":\n",
    "        model.reshape({0: [1, 3, 640, 640]})\n",
    "    if \"GPU\" in device or (\"AUTO\" in device and \"GPU\" in core.available_devices):\n",
    "        ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "    compiled_model = core.compile_model(model, device, ov_config)\n",
    "\n",
    "    def infer(*args):\n",
    "        result = compiled_model(args)\n",
    "        return torch.from_numpy(result[0])\n",
    "\n",
    "    det_model.predictor.inference = infer\n",
    "    det_model.predictor.model.pt = False\n",
    "\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = VideoPlayer(source=source, flip=flip, fps=60, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        frame = player.next()\n",
    "        # Mirror initial frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        if use_popup:\n",
    "            original_frame = frame.copy()\n",
    "            cv2.namedWindow(\"Video\", flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)            \n",
    "            cv2.setMouseCallback(\"Video\",draw_rectangle)\n",
    "            # Display the first frame and wait for the user to draw the rectangle\n",
    "            cv2.imshow(\"Video\", frame)\n",
    "            cv2.waitKey(0)  # Wait until any key is pressed\n",
    "\n",
    "        processing_times = collections.deque()   \n",
    "        \n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            # Mirror image\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            \n",
    "            \n",
    "            if len(rect_points) == 2:\n",
    "                cv2.rectangle(frame, rect_points[0], rect_points[1], (0, 255, 0), 2)\n",
    "                # Crop the ROI\n",
    "                roi = frame[max(0, rect_points[0][1]):rect_points[1][1], max(0, rect_points[0][0]):rect_points[1][0]]\n",
    "            else:\n",
    "                roi = frame.copy()\n",
    "            \n",
    "            # Get the results.\n",
    "            input_image = np.array(roi)\n",
    "\n",
    "            # N,C,H,W = batch size, number of channels, height, width.\n",
    "            N, C, H, W = text_detection_input_layer.shape\n",
    "            \n",
    "            # Resize the image to meet network expected input sizes.\n",
    "            resized_image = cv2.resize(roi, (W, H))\n",
    "            \n",
    "            # Reshape to the network input shape.\n",
    "            text_input_img = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "            start_time = time.time()\n",
    "            # Object detection yolov8 inference\n",
    "            detections = det_model(input_image)\n",
    "            \n",
    "            # Text detection & recognition inference        \n",
    "            boxes = text_detection_compiled_model([text_input_img])[text_output_key] \n",
    "             # Remove zero only boxes.\n",
    "            boxes = boxes[~np.all(boxes == 0, axis=1)]\n",
    "             # Recognition\n",
    "            boxes_with_annotations = text_recognition_inferece(roi,resized_image,boxes)\n",
    "            # End inference\n",
    "            stop_time = time.time()\n",
    "\n",
    "            # Plotting results\n",
    "            roi = detections[0].plot()\n",
    "            roi = convert_result_to_image(roi,resized_image, boxes_with_annotations, conf_labels=True)\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            # Calculate mean processing time and FPS\n",
    "            f_height, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time \n",
    "\n",
    "            target_img = frame if use_popup else roi\n",
    "            cv2.putText(\n",
    "                img=target_img,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )    \n",
    "            \n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                frame[max(0, rect_points[0][1]):rect_points[1][1], max(0, rect_points[0][0]):rect_points[1][0]] = roi\n",
    "                cv2.rectangle(target_img, rect_points[0], rect_points[1], (0, 255, 0), 2)\n",
    "                cv2.imshow(\"Video\",frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Convert to JPEG for display in Jupyter notebook.                \n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=roi, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                i = display.Image(data=encoded_img.tobytes())\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5b4ba6-f478-4417-b09d-93fee5adca41",
   "metadata": {},
   "source": [
    "### Run Live Object Detection\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Use a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`.\n",
    "\n",
    ">**NOTE**: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a remote server (for example, in Binder or Google Colab service), the webcam will not work. By default, the lower cell will run model inference on a video file. If you want to try live inference on your webcam set `WEBCAM_INFERENCE = True`\n",
    "\n",
    "Run the object detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90708017",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM_INFERENCE = True\n",
    "\n",
    "if WEBCAM_INFERENCE:\n",
    "    VIDEO_SOURCE = 2  # Webcam\n",
    "else:\n",
    "    VIDEO_SOURCE = \"https://github.com/antoniomtz/sample-clips/raw/main/barcode.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee30d986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306ff26e560940d89667be01aad42a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'GPU', 'AUTO'), value='GPU')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8de91e4d-321f-46fe-a1ad-425e2a04b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 20.5ms\n",
      "Speed: 3.7ms preprocess, 20.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.8ms\n",
      "Speed: 2.1ms preprocess, 16.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 19.8ms\n",
      "Speed: 2.3ms preprocess, 19.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 18.4ms\n",
      "Speed: 2.9ms preprocess, 18.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 18.5ms\n",
      "Speed: 2.3ms preprocess, 18.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 18.1ms\n",
      "Speed: 2.8ms preprocess, 18.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.8ms\n",
      "Speed: 2.2ms preprocess, 16.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.9ms\n",
      "Speed: 2.1ms preprocess, 16.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.6ms\n",
      "Speed: 2.3ms preprocess, 17.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 37.3ms\n",
      "Speed: 2.2ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.5ms\n",
      "Speed: 2.6ms preprocess, 17.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.6ms\n",
      "Speed: 2.2ms preprocess, 17.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.5ms\n",
      "Speed: 2.2ms preprocess, 17.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.5ms\n",
      "Speed: 2.4ms preprocess, 17.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.3ms\n",
      "Speed: 2.1ms preprocess, 17.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.4ms\n",
      "Speed: 2.1ms preprocess, 17.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 18.1ms\n",
      "Speed: 2.1ms preprocess, 18.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.4ms\n",
      "Speed: 2.3ms preprocess, 17.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 19.8ms\n",
      "Speed: 2.2ms preprocess, 19.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.3ms\n",
      "Speed: 2.2ms preprocess, 17.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 17.2ms\n",
      "Speed: 2.1ms preprocess, 17.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 19.2ms\n",
      "Speed: 2.1ms preprocess, 19.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.9ms\n",
      "Speed: 2.1ms preprocess, 16.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.2ms\n",
      "Speed: 2.4ms preprocess, 16.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.1ms\n",
      "Speed: 2.1ms preprocess, 16.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 15.9ms\n",
      "Speed: 2.8ms preprocess, 15.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 19.4ms\n",
      "Speed: 2.1ms preprocess, 19.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 20.1ms\n",
      "Speed: 2.1ms preprocess, 20.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 18.7ms\n",
      "Speed: 2.2ms preprocess, 18.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 18.9ms\n",
      "Speed: 2.1ms preprocess, 18.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 16.0ms\n",
      "Speed: 2.2ms preprocess, 16.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 17.7ms\n",
      "Speed: 2.3ms preprocess, 17.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 16.2ms\n",
      "Speed: 2.1ms preprocess, 16.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 17.2ms\n",
      "Speed: 2.1ms preprocess, 17.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 15.6ms\n",
      "Speed: 2.2ms preprocess, 15.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 14.8ms\n",
      "Speed: 2.0ms preprocess, 14.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 16.5ms\n",
      "Speed: 2.4ms preprocess, 16.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 16.0ms\n",
      "Speed: 2.8ms preprocess, 16.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 16.0ms\n",
      "Speed: 2.1ms preprocess, 16.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 16.1ms\n",
      "Speed: 2.6ms preprocess, 16.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 17.0ms\n",
      "Speed: 2.2ms preprocess, 17.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 15.1ms\n",
      "Speed: 2.2ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 15.9ms\n",
      "Speed: 2.2ms preprocess, 15.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 16.8ms\n",
      "Speed: 2.1ms preprocess, 16.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 14.7ms\n",
      "Speed: 2.2ms preprocess, 14.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 14.9ms\n",
      "Speed: 2.2ms preprocess, 14.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 15.8ms\n",
      "Speed: 2.5ms preprocess, 15.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 16.2ms\n",
      "Speed: 2.2ms preprocess, 16.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 18.6ms\n",
      "Speed: 2.1ms preprocess, 18.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 1 dining table, 15.4ms\n",
      "Speed: 2.4ms preprocess, 15.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 1 apple, 1 dining table, 14.7ms\n",
      "Speed: 2.1ms preprocess, 14.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 1 apple, 1 dining table, 19.2ms\n",
      "Speed: 2.1ms preprocess, 19.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 16.4ms\n",
      "Speed: 2.1ms preprocess, 16.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 18.3ms\n",
      "Speed: 2.7ms preprocess, 18.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 16.0ms\n",
      "Speed: 2.1ms preprocess, 16.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 18.6ms\n",
      "Speed: 2.2ms preprocess, 18.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 14.9ms\n",
      "Speed: 2.2ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 17.1ms\n",
      "Speed: 2.4ms preprocess, 17.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 15.8ms\n",
      "Speed: 2.8ms preprocess, 15.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 1 banana, 2 apples, 15.3ms\n",
      "Speed: 2.1ms preprocess, 15.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 19.1ms\n",
      "Speed: 2.0ms preprocess, 19.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 14.8ms\n",
      "Speed: 2.2ms preprocess, 14.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 16.1ms\n",
      "Speed: 2.4ms preprocess, 16.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 17.4ms\n",
      "Speed: 2.3ms preprocess, 17.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 18.1ms\n",
      "Speed: 2.1ms preprocess, 18.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 14.6ms\n",
      "Speed: 2.2ms preprocess, 14.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 16.3ms\n",
      "Speed: 2.2ms preprocess, 16.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 18.8ms\n",
      "Speed: 2.0ms preprocess, 18.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 16.6ms\n",
      "Speed: 2.5ms preprocess, 16.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 apple, 1 dining table, 15.7ms\n",
      "Speed: 2.2ms preprocess, 15.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 dining table, 15.4ms\n",
      "Speed: 2.3ms preprocess, 15.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 dining table, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 dining table, 15.5ms\n",
      "Speed: 2.5ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 dining table, 16.3ms\n",
      "Speed: 2.2ms preprocess, 16.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 dining table, 15.0ms\n",
      "Speed: 2.2ms preprocess, 15.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 dining table, 15.0ms\n",
      "Speed: 2.4ms preprocess, 15.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 bottle, 1 cup, 1 dining table, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bottle, 1 cup, 1 dining table, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bottle, 2 cups, 1 dining table, 16.7ms\n",
      "Speed: 2.3ms preprocess, 16.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 banana, 1 dining table, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 banana, 1 dining table, 16.8ms\n",
      "Speed: 2.0ms preprocess, 16.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 banana, 1 dining table, 14.8ms\n",
      "Speed: 2.2ms preprocess, 14.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 banana, 1 dining table, 16.8ms\n",
      "Speed: 1.9ms preprocess, 16.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 banana, 1 dining table, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 banana, 1 dining table, 19.4ms\n",
      "Speed: 2.0ms preprocess, 19.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bottle, 1 cup, 1 banana, 1 dining table, 15.2ms\n",
      "Speed: 2.4ms preprocess, 15.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bottle, 2 cups, 1 banana, 1 dining table, 18.9ms\n",
      "Speed: 2.0ms preprocess, 18.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 18.8ms\n",
      "Speed: 2.1ms preprocess, 18.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 19.2ms\n",
      "Speed: 2.2ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 16.0ms\n",
      "Speed: 2.3ms preprocess, 16.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 16.4ms\n",
      "Speed: 2.3ms preprocess, 16.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 15.4ms\n",
      "Speed: 2.1ms preprocess, 15.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 18.6ms\n",
      "Speed: 2.1ms preprocess, 18.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 14.9ms\n",
      "Speed: 2.1ms preprocess, 14.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 18.0ms\n",
      "Speed: 2.1ms preprocess, 18.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 15.3ms\n",
      "Speed: 2.9ms preprocess, 15.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 19.7ms\n",
      "Speed: 2.2ms preprocess, 19.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 16.8ms\n",
      "Speed: 2.4ms preprocess, 16.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 2 apples, 19.5ms\n",
      "Speed: 2.2ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 17.3ms\n",
      "Speed: 2.2ms preprocess, 17.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 17.6ms\n",
      "Speed: 1.9ms preprocess, 17.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 15.9ms\n",
      "Speed: 2.1ms preprocess, 15.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 17.8ms\n",
      "Speed: 2.1ms preprocess, 17.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 16.3ms\n",
      "Speed: 2.0ms preprocess, 16.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 16.2ms\n",
      "Speed: 2.0ms preprocess, 16.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 1 apple, 16.5ms\n",
      "Speed: 2.3ms preprocess, 16.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 cup, 1 apple, 1 dining table, 18.9ms\n",
      "Speed: 1.9ms preprocess, 18.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 cup, 1 dining table, 14.7ms\n",
      "Speed: 2.3ms preprocess, 14.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 1 apple, 1 dining table, 16.6ms\n",
      "Speed: 1.9ms preprocess, 16.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 2 apples, 1 dining table, 17.1ms\n",
      "Speed: 2.3ms preprocess, 17.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 18.6ms\n",
      "Speed: 2.0ms preprocess, 18.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 15.5ms\n",
      "Speed: 2.5ms preprocess, 15.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 19.1ms\n",
      "Speed: 2.1ms preprocess, 19.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 15.2ms\n",
      "Speed: 2.0ms preprocess, 15.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 16.4ms\n",
      "Speed: 2.2ms preprocess, 16.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 19.1ms\n",
      "Speed: 2.0ms preprocess, 19.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 16.2ms\n",
      "Speed: 2.1ms preprocess, 16.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 apple, 1 dining table, 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 cup, 1 banana, 1 apple, 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 16.9ms\n",
      "Speed: 2.1ms preprocess, 16.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.4ms\n",
      "Speed: 2.1ms preprocess, 15.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.2ms\n",
      "Speed: 2.3ms preprocess, 15.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.3ms\n",
      "Speed: 2.2ms preprocess, 15.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 14.7ms\n",
      "Speed: 2.1ms preprocess, 14.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.4ms\n",
      "Speed: 2.2ms preprocess, 15.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.3ms\n",
      "Speed: 2.2ms preprocess, 15.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.8ms\n",
      "Speed: 2.2ms preprocess, 15.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 16.3ms\n",
      "Speed: 2.1ms preprocess, 16.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.9ms\n",
      "Speed: 2.1ms preprocess, 15.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 17.5ms\n",
      "Speed: 2.3ms preprocess, 17.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 14.7ms\n",
      "Speed: 2.0ms preprocess, 14.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 14.7ms\n",
      "Speed: 2.0ms preprocess, 14.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 14.8ms\n",
      "Speed: 2.1ms preprocess, 14.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.0ms\n",
      "Speed: 1.9ms preprocess, 15.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 17.4ms\n",
      "Speed: 2.1ms preprocess, 17.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 15.2ms\n",
      "Speed: 2.0ms preprocess, 15.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 16.9ms\n",
      "Speed: 2.2ms preprocess, 16.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 14.7ms\n",
      "Speed: 1.9ms preprocess, 14.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 17.7ms\n",
      "Speed: 2.4ms preprocess, 17.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 16.2ms\n",
      "Speed: 2.1ms preprocess, 16.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 17.3ms\n",
      "Speed: 2.1ms preprocess, 17.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 cup, 1 banana, 1 apple, 1 dining table, 14.9ms\n",
      "Speed: 2.3ms preprocess, 14.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "det_ov_model = core.read_model(det_model_path)\n",
    "\n",
    "run_object_detection(\n",
    "    source=VIDEO_SOURCE,\n",
    "    flip=True,\n",
    "    use_popup=True,\n",
    "    skip_first_frames=0,\n",
    "    model=det_ov_model,\n",
    "    device=device.value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065d6b6-6313-402c-8457-f736364c4b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743f5fe-ab8d-4bf0-aecd-87a086f504c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/212105105-f61c8aab-c1ff-40af-a33f-d0ed1fccc72e.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Object Detection"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
